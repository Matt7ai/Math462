{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gos71f9KvmyG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n"
      ],
      "metadata": {
        "id": "JWjC5H1_wC3D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the bias term for every instance\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
        "\n",
        "# Splitting the dataset manually\n",
        "np.random.seed(42)\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "Ybe5hqXewHq5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting class indices to one-hot vectors\n",
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1\n",
        "    m = len(y)\n",
        "    Y_one_hot = np.zeros((m, n_classes))\n",
        "    Y_one_hot[np.arange(m), y] = 1\n",
        "    return Y_one_hot\n",
        "\n",
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "metadata": {
        "id": "_RxggOdVwLuU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train[:, 1:])\n",
        "X_valid_scaled = scaler.transform(X_valid[:, 1:])\n",
        "X_test_scaled = scaler.transform(X_test[:, 1:])"
      ],
      "metadata": {
        "id": "F5MlWex4wOx9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reassign the scaled data keeping the bias term unchanged\n",
        "X_train[:, 1:] = X_train_scaled\n",
        "X_valid[:, 1:] = X_valid_scaled\n",
        "X_test[:, 1:] = X_test_scaled"
      ],
      "metadata": {
        "id": "5GnOP17ywTN9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax function\n",
        "def softmax(logits):\n",
        "    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # to prevent overflow\n",
        "    exp_sums = exps.sum(axis=1, keepdims=True)\n",
        "    return exps / exp_sums"
      ],
      "metadata": {
        "id": "f9u1yYiMwgvo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with Batch Gradient Descent\n",
        "n_inputs = X_train.shape[1]\n",
        "n_outputs = len(np.unique(y_train))\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "eta = 0.1\n",
        "n_epochs = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error)\n",
        "    Theta -= eta * gradients\n",
        "\n",
        "    logits_valid = X_valid.dot(Theta)\n",
        "    Y_proba_valid = softmax(logits_valid)\n",
        "    loss_valid = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba_valid + epsilon), axis=1))\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(epoch, loss_valid)\n",
        "\n",
        "    if loss_valid < best_loss:\n",
        "        best_loss = loss_valid\n",
        "    else:\n",
        "        print(\"Early stopping!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxDvIsEIwjPw",
        "outputId": "6b7e70a1-9fc8-47a9-9b20-aa8adde7e5a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3.862442613706157\n",
            "500 0.10445253119329116\n",
            "1000 0.08923322010615192\n",
            "1500 0.08320593556823673\n",
            "2000 0.08018307790883639\n",
            "2500 0.07850098329889886\n",
            "3000 0.07752243502780187\n",
            "3500 0.07695289523072012\n",
            "4000 0.07663838879619818\n",
            "4500 0.07649105139117111\n",
            "Early stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "logits_test = X_test.dot(Theta)\n",
        "Y_proba_test = softmax(logits_test)\n",
        "y_predict = np.argmax(Y_proba_test, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_test)\n",
        "print(\"Test accuracy: \", accuracy_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxpVL_2Ywn6v",
        "outputId": "a9ec932e-ca13-4119-926c-56e5e83a5490"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy:  0.9714285714285714\n"
          ]
        }
      ]
    }
  ]
}