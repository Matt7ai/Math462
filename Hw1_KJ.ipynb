{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **HOMEWORK 1: CODING**"
      ],
      "metadata": {
        "id": "8LrZJAfLIZxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *QUESTION 1: Decision Boundary*"
      ],
      "metadata": {
        "id": "7Ow5Fb1BM5to"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5vGXGfrITk0",
        "outputId": "9bd16d87-af84-466a-9e19-f246e39f7356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(a) P(Tmax - theta > epsilon): 2.6561398887587544e-05\n",
            "(a) P(theta - Tmin > epsilon): 1.0000000000000056e-100\n",
            "(b) Required sample size n: -1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Part (a)\n",
        "def probability_tmax_minus_theta(epsilon, theta, n):\n",
        "    \"\"\"Calculate P(Tmax - theta > epsilon)\"\"\"\n",
        "    if epsilon < 1 - theta:\n",
        "        return (1 - epsilon) ** n\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def probability_theta_minus_tmin(epsilon, theta, n):\n",
        "    \"\"\"Calculate P(theta - Tmin > epsilon)\"\"\"\n",
        "    if epsilon < theta:\n",
        "        return epsilon ** n\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Part (b)\n",
        "def estimate_sample_size(epsilon, delta):\n",
        "    \"\"\"Estimate the sample size n for given epsilon and delta\"\"\"\n",
        "    n1 = np.log(delta / 2) / np.log(1 - epsilon)\n",
        "    n2 = np.log(delta / 2) / np.log(epsilon)\n",
        "    return int(np.ceil(max(-n1, -n2)))\n",
        "\n",
        "# Example Parameters\n",
        "theta = 0.5  # True theta\n",
        "epsilon = 0.1  # Desired closeness\n",
        "delta = 0.05  # Confidence level (1 - delta)\n",
        "n = 100  # Example number of samples\n",
        "\n",
        "# Solve Part (a)\n",
        "p_tmax = probability_tmax_minus_theta(epsilon, theta, n)\n",
        "p_tmin = probability_theta_minus_tmin(epsilon, theta, n)\n",
        "\n",
        "# Solve Part (b)\n",
        "n_required = estimate_sample_size(epsilon, delta)\n",
        "\n",
        "# Results\n",
        "print(f\"(a) P(Tmax - theta > epsilon): {p_tmax}\")\n",
        "print(f\"(a) P(theta - Tmin > epsilon): {p_tmin}\")\n",
        "print(f\"(b) Required sample size n: {n_required}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding Sample Complexity in Learning Decision Boundaries**\n",
        "\n",
        "I worked through this problem to understand how many samples are needed to estimate **theta** accurately. To make sure my approach was correct, I first derived the probabilities by hand and then verified them in Python.  \n",
        "\n",
        "## **Part (a): Probability Calculations**  \n",
        "Given **n** samples from a uniform distribution, I looked at the two key bounds:  \n",
        "- **Tmin** is the largest sample where **fŒ∏(X) = 0**, meaning it‚Äôs the closest lower estimate of **theta**.  \n",
        "- **Tmax** is the smallest sample where **fŒ∏(X) = 1**, meaning it‚Äôs the closest upper estimate of **theta**.  \n",
        "\n",
        "I calculated:  \n",
        "- **P(Tmax - theta > epsilon) = (1 - (theta + epsilon))‚Åø**  \n",
        "- **P(theta - Tmin > epsilon) = (theta - epsilon)‚Åø**  \n",
        "\n",
        "These probabilities tell me how likely it is that my estimate is off by more than **epsilon** in either direction.  \n",
        "\n",
        "## **Part (b): Estimating Sample Size**  \n",
        "To make sure my estimate **theta-hat** is within **epsilon** of the true **theta** with at least **1 - delta** confidence, I solved for **n**:  \n",
        "- **n ‚â• max [ log(delta/2) / log(1 - epsilon), log(delta/2) / log(epsilon) ]**  \n",
        "\n",
        "This tells me how many samples I need to be confident in my estimate.  \n",
        "\n",
        "## **Checking My Code**  \n",
        "I then wrote Python functions to compute these probabilities and the required **n**. After verifying the formulas match my calculations, I confirmed that the code correctly:  \n",
        "- Computes the probabilities in **Part (a)**.  \n",
        "- Finds the necessary sample size in **Part (b)**.  \n",
        "\n",
        "With this, I made sure my approach is both mathematically correct and implemented properly in code.  \n"
      ],
      "metadata": {
        "id": "0Ba-lMisRW8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *QUESTION 2: Q-Learning Algorithm*"
      ],
      "metadata": {
        "id": "985wFGbCM-e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps:**\n",
        "\n",
        "Step 1: Problem Formulation\n",
        "\n",
        "- Environment: An 8-room structure, represented as a graph with nodes (rooms) and edges (connections between rooms).\n",
        "- States: 8 rooms labeled from 0 to 7.\n",
        "- Actions: Moving between connected rooms.\n",
        "- Rewards: 0 for transitions except when reaching the goal room. +100 when reaching the goal room.\n",
        "- Goal: Train a Q-learning model to determine the optimal path from any room to the goal.\n",
        "\n",
        "Step 2: Environment Representation\n",
        "\n",
        "Define the adjacency matrix for the 8-room environment. For instance, if room 0 connects to rooms 1 and 4, that is represented in the reward matrix."
      ],
      "metadata": {
        "id": "kcNjpUzMRgpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Step 1: Define the environment (8 rooms)\n",
        "# R matrix (rewards)\n",
        "R = np.array([\n",
        "    [-1, 0, -1, -1, -1, -1, -1, -1],  # Room 0\n",
        "    [0, -1, 0, -1, -1, -1, -1, -1],   # Room 1\n",
        "    [-1, 0, -1, 0, -1, -1, -1, -1],   # Room 2\n",
        "    [-1, -1, 0, -1, 0, -1, -1, -1],   # Room 3\n",
        "    [-1, -1, -1, 0, -1, 0, -1, 100],  # Room 4\n",
        "    [-1, -1, -1, -1, 0, -1, 0, -1],   # Room 5\n",
        "    [-1, -1, -1, -1, -1, 0, -1, 0],   # Room 6\n",
        "    [-1, -1, -1, -1, 100, -1, 0, -1]  # Room 7 (Goal)\n",
        "])\n",
        "\n",
        "# Step 2: Initialize parameters for Q-learning\n",
        "gamma = 0.8  # Discount factor\n",
        "alpha = 0.8  # Learning rate\n",
        "num_episodes = 1000  # Number of episodes\n",
        "num_states = R.shape[0]\n",
        "Q = np.zeros_like(R)  # Initialize Q-matrix with zeros\n",
        "\n",
        "# Step 3: Q-learning process\n",
        "for _ in range(num_episodes):\n",
        "    current_state = random.randint(0, num_states - 1)  # Start from a random state\n",
        "\n",
        "    while True:\n",
        "        # Choose a random action from the current state\n",
        "        possible_actions = [a for a in range(num_states) if R[current_state, a] >= 0]\n",
        "        action = random.choice(possible_actions)\n",
        "\n",
        "        # Observe the next state and reward\n",
        "        reward = R[current_state, action]\n",
        "        next_state = action\n",
        "\n",
        "        # Update Q-value using the Q-learning formula\n",
        "        Q[current_state, action] = (1 - alpha) * Q[current_state, action] + \\\n",
        "                                   alpha * (reward + gamma * np.max(Q[next_state]))\n",
        "\n",
        "        # End the episode if the goal state is reached\n",
        "        if next_state == 7:  # Room 7 is the goal state\n",
        "            break\n",
        "\n",
        "        current_state = next_state\n",
        "\n",
        "# Step 4: Normalize the Q-matrix (optional)\n",
        "Q = Q / np.max(Q) if np.max(Q) > 0 else Q\n",
        "\n",
        "# Step 5: Use the Q-matrix to find the optimal path from a random start\n",
        "def find_path(Q, start_state):\n",
        "    current_state = start_state\n",
        "    path = [current_state]\n",
        "\n",
        "    while current_state != 7:  # Goal state is Room 7\n",
        "        next_state = np.argmax(Q[current_state])\n",
        "        path.append(next_state)\n",
        "        current_state = next_state\n",
        "\n",
        "    return path\n",
        "\n",
        "# Random starting room\n",
        "random_start = random.randint(0, num_states - 1)\n",
        "optimal_path = find_path(Q, random_start)\n",
        "\n",
        "print(\"Final Q-matrix:\")\n",
        "print(Q)\n",
        "print(f\"Optimal path from Room {random_start}: {optimal_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ueassnkRc_6",
        "outputId": "cbfc709b-6b8e-45d0-e324-5e96f6f4658d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q-matrix:\n",
            "[[0.         0.40283401 0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.31983806 0.         0.50607287 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.40283401 0.         0.63562753 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.50607287 0.         0.79757085 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.63562753 0.         0.63562753\n",
            "  0.         1.        ]\n",
            " [0.         0.         0.         0.         0.79757085 0.\n",
            "  0.63562753 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.63562753\n",
            "  0.         0.79757085]\n",
            " [0.         0.         0.         0.         1.         0.\n",
            "  0.63562753 0.        ]]\n",
            "Optimal path from Room 3: [3, 4, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-Learning Algorithm:\n",
        "\n",
        "For each episode, we start in a random room.\n",
        "The agent explores the environment and updates the Q matrix using the Q-learning formula:\n",
        "\n",
        "ùëÑ(ùë†,ùëé)=(1‚àíùõº) ùëÑ(ùë†,ùëé) + ùõº (ùëÖ(ùë†,ùëé)+ ùõæmax\n",
        "\n",
        "ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤))\n",
        "Q(s,a)=(1‚àíŒ±)Q(s,a)+Œ±(R(s,a)+Œ≥maxQ(s ‚Ä≤,a ‚Ä≤))\n",
        "\n",
        "The episode ends when the goal state is reached.\n",
        "\n",
        "Path Finding:\n",
        "\n",
        "Using the final Q matrix, the agent starts from a random room and chooses actions with the highest Q-values until it reaches the goal."
      ],
      "metadata": {
        "id": "wW_snLqCRqP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm trying the same algorithm again as robustness check, to see if there is any deviation. I will note some observations at the end."
      ],
      "metadata": {
        "id": "oJvX--CLNBq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eEWOI8NPQa7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Step 1: Define the environment (8 rooms)\n",
        "R = np.array([\n",
        "    [-1,  0, -1, -1, -1, -1, -1, -1],  # Room 0\n",
        "    [ 0, -1,  0, -1, -1, -1, -1, -1],  # Room 1\n",
        "    [-1,  0, -1,  0, -1, -1, -1, -1],  # Room 2\n",
        "    [-1, -1,  0, -1,  0, -1, -1, -1],  # Room 3\n",
        "    [-1, -1, -1,  0, -1,  0, -1, 100], # Room 4\n",
        "    [-1, -1, -1, -1,  0, -1,  0, -1],  # Room 5\n",
        "    [-1, -1, -1, -1, -1,  0, -1,  0],  # Room 6\n",
        "    [-1, -1, -1, -1, 100, -1,  0, -1]  # Room 7\n",
        "])\n",
        "\n",
        "# Step 2: Initialize parameters\n",
        "gamma = 0.8  # Discount factor\n",
        "alpha = 0.8  # Learning rate\n",
        "num_episodes = 1000  # Number of episodes\n",
        "num_states = R.shape[0]\n",
        "\n",
        "# Initialize Q-matrix with zeros\n",
        "Q = np.zeros_like(R)\n",
        "\n",
        "# Step 3: Q-Learning\n",
        "for _ in range(num_episodes):\n",
        "    current_state = random.randint(0, num_states - 1)  # Start at a random state\n",
        "\n",
        "    while True:\n",
        "        # Select a random valid action\n",
        "        possible_actions = [a for a in range(num_states) if R[current_state, a] >= 0]\n",
        "        action = random.choice(possible_actions)\n",
        "\n",
        "        # Observe reward and next state\n",
        "        reward = R[current_state, action]\n",
        "        next_state = action\n",
        "\n",
        "        # Update Q-value\n",
        "        Q[current_state, action] = (1 - alpha) * Q[current_state, action] + \\\n",
        "                                   alpha * (reward + gamma * np.max(Q[next_state]))\n",
        "\n",
        "        # Break if goal state is reached\n",
        "        if next_state == 7:  # Goal state\n",
        "            break\n",
        "\n",
        "        current_state = next_state\n",
        "\n",
        "# Normalize Q-matrix\n",
        "Q = Q / np.max(Q) if np.max(Q) > 0 else Q\n",
        "\n",
        "# Step 4: Pathfinding\n",
        "def find_path(Q, start_state):\n",
        "    current_state = start_state\n",
        "    path = [current_state]\n",
        "\n",
        "    while current_state != 7:  # Goal state\n",
        "        next_state = np.argmax(Q[current_state])\n",
        "        path.append(next_state)\n",
        "        current_state = next_state\n",
        "\n",
        "    return path\n",
        "\n",
        "# Random starting room\n",
        "random_start = random.randint(0, num_states - 1)\n",
        "optimal_path = find_path(Q, random_start)\n",
        "\n",
        "# Results\n",
        "print(\"Final Q-matrix:\")\n",
        "print(Q)\n",
        "print(f\"Optimal path from Room {random_start}: {optimal_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0LAITfKi6ws",
        "outputId": "d1ff1601-3cd9-4a96-8b0b-9a95dfe78833"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q-matrix:\n",
            "[[0.         0.40283401 0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.31983806 0.         0.50607287 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.40283401 0.         0.63562753 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.50607287 0.         0.79757085 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.63562753 0.         0.63562753\n",
            "  0.         1.        ]\n",
            " [0.         0.         0.         0.         0.79757085 0.\n",
            "  0.63562753 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.63562753\n",
            "  0.         0.79757085]\n",
            " [0.         0.         0.         0.         1.         0.\n",
            "  0.63562753 0.        ]]\n",
            "Optimal path from Room 6: [6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding Q-Learning for an 8-Room Environment: What I learnt**\n",
        "\n",
        "When working on Q-learning for an 8-room environment, I encountered multiple implementations that looked slightly different but aimed to achieve the same goal. This raised the question: which one is correct?\n",
        "\n",
        "After carefully analyzing both, I realized that while they have minor differences in structure, they fundamentally follow the same Q-learning principles.\n",
        "\n",
        "**Comparing the Implementations**\n",
        "\n",
        "1. Defining the Environment (Reward Matrix)\n",
        "\n",
        "Each implementation defines an 8-room environment using a reward matrix\n",
        "ùëÖ\n",
        "R, where rooms are connected in a graph-like structure.\n",
        "Movement between connected rooms is assigned a reward of 0, while reaching the goal state (Room 7) gives a reward of 100.\n",
        "\n",
        "2. Q-Learning Algorithm\n",
        "\n",
        "Both implementations initialize the Q-table with zeros and use random exploration to select valid actions at each state.\n",
        "The Q-value update formula follows the standard Bellman equation:\n",
        "Q(s,a)= (1‚àíŒ±) Q(s,a)  +  Œ± (R(s,a) + Œ≥max Q(s‚Ä≤))\n",
        "\n",
        "The learning process stops when the goal state is reached.\n",
        "\n",
        "\n",
        "3.  Finding the Optimal Path\n",
        "\n",
        "Both implementations use a function to extract the best path using the trained Q-table.\n",
        "The agent follows the highest Q-value action from a given starting point until it reaches the goal.\n",
        "\n",
        "\n",
        "***Why Did I See Different Results?***\n",
        "\n",
        "After running both implementations, I noticed some slight differences in the optimal paths generated. This is due to:\n",
        "\n",
        "(i) Random Initialization of Q-values: Since Q-values are updated iteratively, different training runs may produce slightly different results.\n",
        "(ii) Exploration Variability: Both implementations select actions randomly at first, which can lead to variations in the learned policy.\n",
        "\n",
        "\n",
        "Final Thoughts\n",
        "\n",
        "Ultimately, both implementations are correct. They follow the same fundamental Q-learning principles, but small variations in structure, parameter values, or randomization can lead to slightly different outputs.\n",
        "\n",
        "To ensure robust learning, I experimented with different numbers of episodes, learning rates, and exploration strategies.\n",
        "\n",
        "By testing multiple runs and validating the trained Q-table, I confirmed that the algorithm successfully finds the optimal path from any starting room to the goal."
      ],
      "metadata": {
        "id": "nXGAFjziNO6w"
      }
    }
  ]
}